{
  "basics": {
    "name": "Zilin Kang (康梓林)",
    "email": "kzl22@mails.tsinghua.edu.cn",
    "phone": "",
    "website": "https://nothingbutbut.github.io",
    "summary": "Currently an undergraduate student at Tsinghua University, majoring in Computer Science and Technology. I am interested in reinforcement learning and robotics. I have experience in software development, algorithm design, and data analysis.",
    "location": {
      "address": "Tsinghua University, Haidian District",
      "postalCode": "100084",
      "city": "Beijing",
      "countryCode": "CN",
      "region": ""
    },
    "profiles": [
      {
        "network": "Google Scholar",
        "username": "",
        "url": "TBD"
      },
      {
        "network": "ORCID",
        "username": "",
        "url": "TBD"
      },
      {
        "network": "GitHub",
        "username": "nothingbutbut",
        "url": "https://github.com/nothingbutbut"
      }
    ]
  },
  "work": [],
  "education": [
    {
      "institution": "Tsinghua University",
      "area": "B.S. in Computer Science and Technology",
      "studyType": "",
      "startDate": "2022",
      "endDate": "2026",
      "gpa": "3.89/4.0 (ranking 43/187)",
      "courses": []
    }
  ],
  "skills": [],
  "languages": [],
  "interests": [],
  "references": [],
  "publications": [
    {
      "name": "A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control",
      "publisher": "PMLR",
      "releaseDate": "2025",
      "website": "https://arxiv.org/abs/2507.02712",
      "summary": "Deep reinforcement learning for continuous control has recently achieved impressive progress. However, existing methods often suffer from primacy bias, a tendency to overfit early experiences stored in the replay buffer, which limits an RL agent's sample efficiency and generalizability. In contrast, humans are less susceptible to such bias, partly due to infantile amnesia, where the formation of new neurons disrupts early memory traces, leading to the forgetting of initial experiences. Inspired by this dual processes of forgetting and growing in neuroscience, in this paper, we propose Forget and Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First, Experience Replay Decay (ER Decay) \"forgetting early experience\", which balances memory by gradually reducing the influence of early experiences. Second, Network Expansion, \"growing neural capacity\", which enhances agents' capability to exploit the patterns of existing data by dynamically adding new parameters during training. Empirical results on four major continuous control benchmarks with more than 40 tasks demonstrate the superior performance of FoG against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2."
    }
  ]
}